{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CLR.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "13HuqDlTEbz14qOOI7O2RtoMWmsbTewbr",
          "timestamp": 1529350831100
        }
      ],
      "collapsed_sections": [
        "3JA91-RvS4B9"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Vaw-HZ4qwDDk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# INITIALIZATION"
      ]
    },
    {
      "metadata": {
        "id": "DcqzWk0x_RQF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ghEobAV_YHy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/COLAB/MicroCalc\")\n",
        "!ls --full-time\n",
        "\n",
        "!pip install -q scikit-plot\n",
        "# !pip install -q tflearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bDPuYa05w3vi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "zlxA3TZqpAUn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "train_size  = 128000\n",
        "actual_num_pos_train = 3459\n",
        "actual_num_neg_train = train_size - actual_num_pos_train\n",
        "\n",
        "\n",
        "#initially, the rising edge covers the complete experiment\n",
        "fixed_lr = 0.2\n",
        "base_lr = 0.002    #1e-3\n",
        "max_lr = 0.03      #4e-1\n",
        "epochs = 12\n",
        "epochs_per_step = 2\n",
        "batch_size = 32\n",
        "step_size = (int)(((train_size/batch_size))*epochs_per_step) \n",
        "print('confirmed at {}'.format(datetime.datetime.now().time()))\n",
        "\n",
        "model_name = \"kerasModel_fixed.json\"\n",
        "weights_name = \"modelWeights_fixed.h5\"\n",
        "normalizer_name = \"normalizer_fixed.pkl\"\n",
        "policy = 'exp_range'\n",
        "\n",
        "# to save models, weights, and normalizers\n",
        "save = False; "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A09xm2QqqqrP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from CLR import CyclicLR\n",
        "### keras tools\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "## sklearn tools\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import scikitplot as skplt\n",
        "\n",
        "import h5py\n",
        "#################################################################################################################\n",
        "\n",
        "# POSITIVE - TRAIN\n",
        "print(\"version 7\")\n",
        "\n",
        "\n",
        "print(\"step_size = {}\".format(step_size))\n",
        "print('epochs {}, base_lr {}, max_lr {}, batch_size {}, epochs_per_step {}'.format(epochs, base_lr, max_lr, batch_size, epochs_per_step))\n",
        "fpos = h5py.File('Positive_train.h5', 'r')\n",
        "first_key=list(fpos.keys())[0]\n",
        "dpos=fpos[first_key]\n",
        "size_image=dpos.shape[1]\n",
        "num_pos_train = dpos.shape[0]\n",
        "dposar = np.zeros(dpos.shape)\n",
        "dpos.read_direct(dposar)\n",
        "train_pos=dposar.reshape([-1,size_image,size_image,1])\n",
        "\n",
        "print(\"Total positive patches for training are: \", num_pos_train)\n",
        "\n",
        "# NEGATIVE - TRAIN\n",
        "fneg = h5py.File('300kNegative_train.h5', 'r')\n",
        "first_key=list(fneg.keys())[0]\n",
        "dneg=fneg[first_key]\n",
        "size_image=dneg.shape[1]\n",
        "num_neg_train = dneg.shape[0]\n",
        "dnegar = np.zeros(dneg.shape)\n",
        "dneg.read_direct(dnegar)\n",
        "train_neg=dnegar.reshape([-1,size_image,size_image,1])\n",
        "\n",
        "print(\"Total negative patches for training are: \", num_neg_train)\n",
        "print('-----------------------------------------------------------')\n",
        "\n",
        "print(\"Positive patches for training are: \", actual_num_pos_train)\n",
        "print(\"Negative patches for training are: \", actual_num_neg_train)\n",
        "\n",
        "\n",
        "\n",
        "# Create the target vectors\n",
        "train_pos_lab = np.ones((actual_num_pos_train,1))\n",
        "train_neg_lab = np.zeros((actual_num_neg_train,1))\n",
        "\n",
        "#### Build the training set  (images and targets)\n",
        "tpos = np.copy(train_pos[0:actual_num_pos_train,:,:,:])\n",
        "tneg = np.copy(train_neg[0:actual_num_neg_train,:,:,:])\n",
        "\n",
        "# Stack the subsets\n",
        "X_Train = np.vstack((tpos,tneg))\n",
        "Y_Train = np.vstack((train_pos_lab,train_neg_lab))\n",
        "\n",
        "# Shuffle the two arrays in unison\n",
        "X_Train, Y_Train = shuffle(X_Train,Y_Train)\n",
        "\n",
        "######################################################## augmentation + \n",
        "#######################         Normalization\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,\n",
        "    featurewise_std_normalization=True,\n",
        "    rotation_range=0,\n",
        "    vertical_flip=False)\n",
        "datagen.fit(X_Train)\n",
        "\n",
        "################################# Network architecture\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size=(3, 3), activation='relu',padding='same', name='block1_conv1', input_shape = X_Train.shape[1:]))\n",
        "model.add(Conv2D(filters =32, kernel_size=(3, 3), activation='relu',padding='same', name='block1_conv2'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
        "\n",
        "model.add(Conv2D(filters =32, kernel_size=(3, 3), activation='relu',padding='same', name='block2_conv1'))\n",
        "model.add(Conv2D(filters =32, kernel_size=(3, 3), activation='relu',padding='same', name='block2_conv2'))\n",
        "model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(units = 256, activation='relu', name='fc1'))\n",
        "model.add(Dropout(rate=0.5))\n",
        "\n",
        "model.add(Dense(units = 256, activation='relu', name='fc2'))\n",
        "model.add(Dropout(rate=0.5))\n",
        "\n",
        "model.add(Dense(units=2, activation='softmax', name='predictions'))\n",
        "\n",
        "\n",
        "Y_one_hot = to_categorical(np.ravel(Y_Train),2)\n",
        "\n",
        "#### fixed lr part\n",
        "\n",
        "# gradientDescent = SGD(lr= 0.1, decay=0.96)\n",
        "# model.compile(gradientDescent, loss= 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# model.fit_generator(datagen.flow(x= X_Train, y =Y_one_hot,batch_size = batch_size),\n",
        "#                      verbose = 2,epochs=epochs,  use_multiprocessing=True) #steps_per_epoch=len(X_Train) /batch_size,\n",
        "\n",
        "\n",
        "##clr\n",
        "\n",
        "clr_triangular = CyclicLR(mode = policy, base_lr = base_lr, max_lr = max_lr, step_size = step_size, gamma=0.99994)\n",
        "\n",
        "gradientDescent=SGD()\n",
        "model.compile(gradientDescent, loss= 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "#### model.summary()\n",
        "model.fit_generator(datagen.flow(x= X_Train, y =Y_one_hot,batch_size = batch_size),steps_per_epoch=len(X_Train) / batch_size,\n",
        "                    callbacks=[clr_triangular], verbose=2, \n",
        "                    epochs=epochs, use_multiprocessing=True)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.ylabel('Training accuracy')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.title(\"CLR - '{}' Policy\".format(policy))\n",
        "plt.plot(clr_triangular.history['lr'],clr_triangular.history['acc'] )\n",
        "\n",
        "plt.figure(2)\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.title(\"CLR - '{}' Policy\".format(policy))\n",
        "plt.plot(clr_triangular.history['iterations'],clr_triangular.history['lr'] )\n",
        "\n",
        "plt.figure(3)\n",
        "\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title(\"CLR - '{}' Policy\".format(policy))\n",
        "plt.plot(clr_triangular.history['iterations'],clr_triangular.history['acc'] )\n",
        "\n",
        "#####################################################\n",
        "#NORMALIATION    #######################################\n",
        "\n",
        "normalized_Xtrain = datagen.standardize(X_Train)\n",
        "\n",
        "####################################################\n",
        "####################################################\n",
        "y_pred_keras = model.predict_proba(normalized_Xtrain, verbose=2, batch_size=batch_size)\n",
        "fpr_keras, tpr_keras, _ = roc_curve(Y_Train, y_pred_keras[:,1])\n",
        "train_auc = roc_auc_score(Y_Train, y_pred_keras[:,1])\n",
        "\n",
        "train_precision, train_recall, train_f1score,_ = precision_recall_fscore_support(Y_Train, np.argmax(y_pred_keras, axis=1),average = 'binary')\n",
        "print(\"AUC: %.4f PRECISION:  %.4f RECALL: %.4f F1SCORE: %.4f \" %(train_auc,train_precision, train_recall, train_f1score))\n",
        "\n",
        "\n",
        "skplt.metrics.plot_roc(Y_Train, y_pred_keras, plot_micro=False, plot_macro=False, classes_to_plot=1)\n",
        "############################### #serialize model to JSON and weights to HDF5\n",
        "if save == True:\n",
        "    model_json = model.to_json()\n",
        "    with open(model_name, 'w') as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "    model.save_weights(weights_name)\n",
        "    joblib.dump(datagen,normalizer_name)\n",
        "\n",
        "    print(\"model {}, normalizer, and weights have been saved\".format(model_name))\n",
        "\n",
        "###############################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fQwc7j6Awysn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **TEST**"
      ]
    },
    {
      "metadata": {
        "id": "WbXU99q3NLvs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import scikitplot as skplt\n",
        "import h5py\n",
        "\n",
        "\n",
        "#################################################################################################################\n",
        "# POSITIVE - Test\n",
        "fpos = h5py.File('Positive_test.h5', 'r')\n",
        "first_key = list(fpos.keys())[0]\n",
        "dpos=fpos[first_key]\n",
        "size_image=dpos.shape[1]\n",
        "num_pos_test = dpos.shape[0]\n",
        "dposar = np.zeros(dpos.shape)\n",
        "dpos.read_direct(dposar)\n",
        "test_pos=dposar.reshape([-1,size_image,size_image,1])\n",
        "\n",
        "print(\"Total positive patches for testing are: \", num_pos_test)\n",
        "\n",
        "# NEGATIVE - Test\n",
        "fneg = h5py.File('300kNegative_test.h5', 'r')\n",
        "first_key=list(fneg.keys())[0]\n",
        "dneg=fneg[first_key]\n",
        "size_image=dneg.shape[1]\n",
        "num_neg_test = dneg.shape[0]\n",
        "dnegar = np.zeros(dneg.shape)\n",
        "dneg.read_direct(dnegar)\n",
        "test_neg=dnegar.reshape([-1,size_image,size_image,1])\n",
        "\n",
        "print(\"Total negative patches for testing are: \", num_neg_test)\n",
        "\n",
        "# the sum of positive and negative test set ~= 20% od training set size\n",
        "actual_num_pos_test = 1164\n",
        "actual_num_neg_test = 110000\n",
        "\n",
        "# Create the target vectors\n",
        "test_pos_lab = np.ones((actual_num_pos_test,1))\n",
        "test_neg_lab = np.zeros((actual_num_neg_test,1))\n",
        "\n",
        "#### Build the training set  (images and targets)\n",
        "tpos = np.copy(test_pos[0:actual_num_pos_test,:,:,:])\n",
        "tneg = np.copy(test_neg[0:actual_num_neg_test,:,:,:])\n",
        "\n",
        "# Stack the subsets\n",
        "X_Test = np.vstack((tpos,tneg))\n",
        "Y_Test = np.vstack((test_pos_lab,test_neg_lab))\n",
        "\n",
        "json_file = open(model_name, 'r')\n",
        "loaded_json_model = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "loaded_model = model_from_json(loaded_json_model)\n",
        "loaded_model.load_weights(weights_name)\n",
        "\n",
        "print(\"model {} loaded\".format(model_name))\n",
        "gradientDescent = SGD(lr= 0.1, decay=0.96)\n",
        "loaded_model.compile(gradientDescent, loss= 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "####### Loading NORMALIZER #######\n",
        "########                 ################################\n",
        "normalizer  = joblib.load(normalizer_name)\n",
        "normalized_Xtest = normalizer.standardize(X_Test)\n",
        "\n",
        "predictions = loaded_model.predict_proba(normalized_Xtest)\n",
        "Y_one_hot = to_categorical(np.ravel(Y_Test),2)\n",
        "score = loaded_model.evaluate(x=X_Test,y= Y_one_hot)\n",
        "print(\"The Scored loss is %.4f, accuracy %.4f\"%(score[0], score[1]))\n",
        "\n",
        "# print('accuracy ={}'.format(np.mean(np.argmax(predictions,1)==Y_Test)))\n",
        "# print('accuracy ={}'.format(accuracy_score(Y_Test, np.argmax(predictions,1))))\n",
        "\n",
        "fpr_keras, tpr_keras, _ = roc_curve(Y_Test, predictions[:,1])\n",
        "\n",
        "test_auc = roc_auc_score(Y_Test, predictions[:,1])\n",
        "\n",
        "test_precision, test_recall, test_f1score, _ = precision_recall_fscore_support(Y_Test, np.argmax(predictions, axis=1),average = 'binary')\n",
        "\n",
        "print(\"AUC: %.4f PRECISION:  %.4f RECALL: %.4f F1SCORE: %.4f \" %(test_auc,test_precision, test_recall, test_f1score))\n",
        "\n",
        "skplt.metrics.plot_roc(Y_Test, predictions ,plot_micro=False, plot_macro=False, classes_to_plot=1)\n",
        "skplt.metrics.plot_precision_recall(Y_Test, predictions ,plot_micro=False, classes_to_plot=1, cmap='plasma')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "17lxcpQocnKI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "HG25mN6PcKkg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}